---
title: "Christmas Carol analysis"
author: "Steve Simon"
date: "February 11, 2018"
output: html_document
---

```{r pre-processing}
library(dplyr)
library(magrittr)
library(sentimentr)
library(SnowballC)
library(tidytext)
library(tidyverse)
```

```{r read-text}
trim_introduction <- function(f, verbose=TRUE) {
  # ^ and $ tell you that the string must start and end the line.
  # In other words, nothing else besides STAVE ONE.
  start_string <- "^STAVE ONE$"
  start_line <- grep(start_string, f)
  n <- length(f)
  if (verbose) print(f[start_line])
  if (length(start_line)!=1) return(f)
  return(f[start_line:n])
}
trim_end_comments <- function(f, verbose=TRUE) {
  end_string <- "Tiny Tim observed, God bless Us, Every One!"
  end_line <- grep(end_string, f)
  n <- length(f)
  if (verbose) print(f[end_line])
  if (length(end_line)!=1) return(f)
  return(f[1:end_line])
}
trim_duplicate_blank_lines <- function(f, verbose=TRUE) {
  n <- length(f)
  consecutive_blank_lines <- f[-n]=="" & f[-1]==""
  if (verbose) print(which(consecutive_blank_lines))
  return(f[!consecutive_blank_lines])
}

filename <- "https://www.gutenberg.org/files/24022/24022-0.txt"
# cc_raw_text <- readLines(filename)
cc_raw_text <- readLines("~/text-mining-examples/data/24022-0.txt")
cc_raw_text                            %>%
  trim_introduction                    %>%
  trim_end_comments                    %>%
  trim_duplicate_blank_lines           -> cc_trimmed_text
n <- length(cc_trimmed_text)
i <- sample(100:(n-100), 1)
head(cc_trimmed_text, 19)
cc_trimmed_text[i:(i+19)]
tail(cc_trimmed_text, 19)
```

```{r tokenize}
cc_trimmed_text                        %>%
  tibble(line=.)                       %>%
  unnest_tokens(
    input = "line",
    output = "para",
    token="paragraphs")                -> cc_paragraphs
cc_paragraphs$para_num <- 1:dim(cc_paragraphs)[1]
cc_paragraphs                          %>%
  unnest_tokens(
    input = "para",
    output = "sent",
    token = "sentences")               -> cc_sentences
cc_sentences$sent_num <- 1:dim(cc_sentences)[1]
cc_sentences                           %>%
  unnest_tokens(
    input = "sent",
    output = "word",
    token = "words")                   -> cc_words
cc_words                               %>%
  anti_join(stop_words)                %>%
  count(word, sort=TRUE)               -> cc_word_count
library(wordcloud)
wordcloud(cc_word_count$word, cc_word_count$n, max.words=99)
```

```{r sentiment, fig.height=9}
sentiments                             %>%
  filter(sentiment=="fear")            %>%
  right_join(cc_words)                 %>%
  mutate(fear=
    ifelse(is.na(sentiment), 0, 1))    %>%
  select(word, fear, para_num, sent_num) -> cc_fear

stave_para <- grep("^stave", cc_paragraphs$para)
stave_sent <- grep("^stave", cc_sentences$sent)
stave_text <- c(
  "Stave 1\nMarley's\nGhost", 
  "Stave 2\nThe First\nof the\nThree\nSpirits",
  "Stave 3\nThe Second\nof the\nThree\nSpirits",
  "Stave 4\nThe Last\nof the\nSpirits",
  "Stave 5\nThe End\nof It")
para_labels <- data.frame(x=stave_para, y=-0.5, text=stave_text)
sent_labels <- data.frame(x=stave_sent, y=-0.5, text=stave_text)
cc_fear                                 %>%
  group_by(para_num)                    %>%
  summarize(avg_fear=mean(fear))        %>%
  ggplot(aes(para_num, avg_fear))        +
    scale_x_reverse(
      breaks=para_labels$x,
      minor=NULL)                        +
    scale_y_continuous(position="right") +
    xlab("Paragraph")                    +
    ylab("Sentiment (fear)")             +
    coord_flip()                         +
    geom_point(color="gray")             +
    geom_smooth(span = 0.1, se = FALSE)  +
    geom_text(data=para_labels, aes(x, y, label=text), vjust=1, hjust=0) 
  
```

```{r save-everything}
save.image("~/text-mining-examples/data/christmas-carol.RData")
```